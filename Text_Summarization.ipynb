{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Text Summarization Using Keras and Tensorflow Backend**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Importing all the modules\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import newspaper as ns\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Dropout, Dense, LSTM, Activation\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Reading BBC News Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "heading = {}\n",
    "description = {}\n",
    "for root, dirs, files in os.walk('bbc/'):\n",
    "    for key, name in enumerate(files):\n",
    "        if \".txt\" in name:\n",
    "            with open(os.path.join(root, name), \"rb\") as f:\n",
    "                tmp = f.read().splitlines()\n",
    "                heading[key] = tmp[0].decode('utf-8')\n",
    "                description[key] = tmp[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "511"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(heading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "511"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heading:  Mobiles rack up 20 years of use\n",
      "Description:\n",
      " [b'Mobile phones in the UK are celebrating their 20th anniversary this weekend.', b'', b\"Britain's first mobile phone call was made across the Vodafone network on 1 January 1985 by veteran comedian Ernie Wise. In the 20 years since that day, mobile phones have become an integral part of modern life and now almost 90% of Britons own a handset. Mobiles have become so popular that many people use their handset as their only phone and rarely use a landline.\", b'', b\"The first ever call over a portable phone was made in 1973 in New York but it took 10 years for the first commercial mobile service to be launched. The UK was not far behind the rest of the world in setting up networks in 1985 that let people make calls while they walked. The first call was made from St Katherine's dock to Vodafone's head office in Newbury which at the time was over a curry house. For the first nine days of 1985 Vodafone was the only firm with a mobile network in the UK. Then on 10 January Cellnet (now O2) launched its service. Mike Caudwell, spokesman for Vodafone, said that when phones were launched they were the size of a briefcase, cost about \\xc2\\xa32,000 and had a battery life of little more than 20 minutes.\", b'', b'\"Despite that they were hugely popular in the mid-80s,\" he said. \"They became a yuppy must-have and a status symbol among young wealthy business folk.\" This was also despite the fact that the phones used analogue radio signals to communicate which made them very easy to eavesdrop on. He said it took Vodafone almost nine years to rack up its first million customers but only 18 months to get the second million. \"It\\'s very easy to forget that in 1983 when we put the bid document in we were forecasting that the total market would be two million people,\" he said. \"Cellnet was forecasting half that.\" Now Vodafone has 14m customers in the UK alone. Cellnet and Vodafone were the only mobile phone operators in the UK until 1993 when One2One (now T-Mobile) was launched. Orange had its UK launch in 1994. Both newcomers operated digital mobile networks and now all operators use this technology. The analogue spectrum for the old phones has been retired. Called Global System for Mobiles (GSM) this is now the most widely used phone technology on the planet and is used to help more than 1.2 billion people make calls. Mr Caudwell said the advent of digital technology also helped to introduce all those things, such as text messaging and roaming that have made mobiles so popular.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Heading: \", heading[0])\n",
    "print(\"Description:\\n\", description[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for k, v in description.items():\n",
    "    description[k] = \"\".join(w.decode('utf-8') for w in v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mobiles rack up 20 years of use\n",
      "Mobile phones in the UK are celebrating their 20th anniversary this weekend.Britain's first mobile phone call was made across the Vodafone network on 1 January 1985 by veteran comedian Ernie Wise. In the 20 years since that day, mobile phones have become an integral part of modern life and now almost 90% of Britons own a handset. Mobiles have become so popular that many people use their handset as their only phone and rarely use a landline.The first ever call over a portable phone was made in 1973 in New York but it took 10 years for the first commercial mobile service to be launched. The UK was not far behind the rest of the world in setting up networks in 1985 that let people make calls while they walked. The first call was made from St Katherine's dock to Vodafone's head office in Newbury which at the time was over a curry house. For the first nine days of 1985 Vodafone was the only firm with a mobile network in the UK. Then on 10 January Cellnet (now O2) launched its service. Mike Caudwell, spokesman for Vodafone, said that when phones were launched they were the size of a briefcase, cost about £2,000 and had a battery life of little more than 20 minutes.\"Despite that they were hugely popular in the mid-80s,\" he said. \"They became a yuppy must-have and a status symbol among young wealthy business folk.\" This was also despite the fact that the phones used analogue radio signals to communicate which made them very easy to eavesdrop on. He said it took Vodafone almost nine years to rack up its first million customers but only 18 months to get the second million. \"It's very easy to forget that in 1983 when we put the bid document in we were forecasting that the total market would be two million people,\" he said. \"Cellnet was forecasting half that.\" Now Vodafone has 14m customers in the UK alone. Cellnet and Vodafone were the only mobile phone operators in the UK until 1993 when One2One (now T-Mobile) was launched. Orange had its UK launch in 1994. Both newcomers operated digital mobile networks and now all operators use this technology. The analogue spectrum for the old phones has been retired. Called Global System for Mobiles (GSM) this is now the most widely used phone technology on the planet and is used to help more than 1.2 billion people make calls. Mr Caudwell said the advent of digital technology also helped to introduce all those things, such as text messaging and roaming that have made mobiles so popular.\n"
     ]
    }
   ],
   "source": [
    "print(heading[0])\n",
    "print(description[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Scraping content to test the text summarizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'But Calvasina said she believes after studying the comments from more than 200 major companies, there still is not clarity from managements, and the impact on profits is not fully priced in, as a result. \"There\\'s probably more upside risk than downside risk,\" she said.\\n\\nTax reform is one of the factors that has helped the stock market rally into the new year, and the bull still appears unstoppable early on in the earnings season. According to Bespoke, U.S. stock market value, measured by the Russell 3000, crossed $30 trillion for the first time Friday, and it is now up more than $6.5 trillion under President Donald Trump. The Russell 3000 represents about 98.5 percent of the U.S. market cap.\\n\\nThe S&P 500 is up more than 4 percent since just the beginning of the year, having tagged on another 1.5 percent in the past week. The market has been lifted by expectations for a strong economy, and the prospect that earnings will be good and get even better, when the impact of new 21 percent corporate tax rate is factored in.\\n\\nSource: Bespoke\\n\\nPaul Hickey, co-founder of Bespoke, said he sees a warning sign in the earnings this season. \"The pace of positive revisions to negative revisions haven\\'t been higher than at any time in 10 years. That\\'s a function of the tax bill,\" said Hickey. \"This is the first quarter in 13 quarters when there\\'s been more positive revisions in the month leading up to earnings than negative revisions.\"\\n\\nThe concern is that analysts lowered the bar with reduced estimates, ahead of those other earnings reporting seasons, making it easier for stocks to realize an earnings pop, but this time, analysts are raising the bar.\\n\\n\"Early on, in these reports, we want to see stocks reacting well, and so far its been pretty good. It\\'s early,\" said Hickey, adding the real test will come when tech names report. Those have been the best performers of 2016, and the group as a whole is expected to see the least benefit from the tax bill.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site = \"https://www.cnbc.com/2018/01/12/earnings-will-be-the-next-test-for-the-seemingly-unstoppable-30-trillion-stock-market.html\"\n",
    "article = ns.Article(site)\n",
    "article.download()\n",
    "article.parse()\n",
    "text = article.text\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Creating the text corpus for creating vocablary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mobile', 'phones', 'in', 'the', 'UK', 'are', 'celebrating', 'their', '20th', 'anniversary', 'this', 'weekend.Britain', \"'s\", 'first', 'mobile', 'phone', 'call', 'was', 'made', 'across', 'the', 'Vodafone', 'network', 'on', '1', 'January', '1985', 'by', 'veteran', 'comedian', 'Ernie', 'Wise', '.', 'In', 'the', '20', 'years', 'since', 'that', 'day', ',', 'mobile', 'phones', 'have', 'become', 'an', 'integral', 'part', 'of', 'modern', 'life', 'and', 'now', 'almost', '90', '%', 'of', 'Britons', 'own', 'a', 'handset', '.', 'Mobiles', 'have', 'become', 'so', 'popular', 'that', 'many', 'people', 'use', 'their', 'handset', 'as', 'their', 'only', 'phone', 'and', 'rarely', 'use', 'a', 'landline.The', 'first', 'ever', 'call', 'over', 'a', 'portable', 'phone', 'was', 'made', 'in', '1973', 'in', 'New', 'York', 'but', 'it', 'took', '10', 'years', 'for', 'the', 'first', 'commercial', 'mobile', 'service', 'to', 'be', 'launched', '.', 'The', 'UK', 'was', 'not', 'far', 'behind', 'the', 'rest', 'of', 'the', 'world', 'in', 'setting', 'up', 'networks', 'in', '1985', 'that', 'let', 'people', 'make', 'calls', 'while', 'they', 'walked', '.', 'The', 'first', 'call', 'was', 'made', 'from', 'St', 'Katherine', \"'s\", 'dock', 'to', 'Vodafone', \"'s\", 'head', 'office', 'in', 'Newbury', 'which', 'at', 'the', 'time', 'was', 'over', 'a', 'curry', 'house', '.', 'For', 'the', 'first', 'nine', 'days', 'of', '1985', 'Vodafone', 'was', 'the', 'only', 'firm', 'with', 'a', 'mobile', 'network', 'in', 'the', 'UK', '.', 'Then', 'on', '10', 'January', 'Cellnet', '(', 'now', 'O2', ')', 'launched', 'its', 'service', '.', 'Mike', 'Caudwell', ',', 'spokesman', 'for', 'Vodafone', ',', 'said', 'that', 'when', 'phones', 'were', 'launched', 'they', 'were', 'the', 'size', 'of', 'a', 'briefcase', ',', 'cost', 'about', '£2,000', 'and', 'had', 'a', 'battery', 'life', 'of', 'little', 'more', 'than', '20', 'minutes', '.', '``', 'Despite', 'that', 'they', 'were', 'hugely', 'popular', 'in', 'the', 'mid-80s', ',', \"''\", 'he', 'said', '.', '``', 'They', 'became', 'a', 'yuppy', 'must-have', 'and', 'a', 'status', 'symbol', 'among', 'young', 'wealthy', 'business', 'folk', '.', \"''\", 'This', 'was', 'also', 'despite', 'the', 'fact', 'that', 'the', 'phones', 'used', 'analogue', 'radio', 'signals', 'to', 'communicate', 'which', 'made', 'them', 'very', 'easy', 'to', 'eavesdrop', 'on', '.', 'He', 'said', 'it', 'took', 'Vodafone', 'almost', 'nine', 'years', 'to', 'rack', 'up', 'its', 'first', 'million', 'customers', 'but', 'only', '18', 'months', 'to', 'get', 'the', 'second', 'million', '.', '``', 'It', \"'s\", 'very', 'easy', 'to', 'forget', 'that', 'in', '1983', 'when', 'we', 'put', 'the', 'bid', 'document', 'in', 'we', 'were', 'forecasting', 'that', 'the', 'total', 'market', 'would', 'be', 'two', 'million', 'people', ',', \"''\", 'he', 'said', '.', '``', 'Cellnet', 'was', 'forecasting', 'half', 'that', '.', \"''\", 'Now', 'Vodafone', 'has', '14m', 'customers', 'in', 'the', 'UK', 'alone', '.', 'Cellnet', 'and', 'Vodafone', 'were', 'the', 'only', 'mobile', 'phone', 'operators', 'in', 'the', 'UK', 'until', '1993', 'when', 'One2One', '(', 'now', 'T-Mobile', ')', 'was', 'launched', '.', 'Orange', 'had', 'its', 'UK', 'launch', 'in', '1994', '.', 'Both', 'newcomers', 'operated', 'digital', 'mobile', 'networks', 'and', 'now', 'all', 'operators', 'use', 'this', 'technology', '.', 'The', 'analogue', 'spectrum', 'for', 'the', 'old', 'phones', 'has', 'been', 'retired', '.', 'Called', 'Global', 'System', 'for', 'Mobiles', '(', 'GSM', ')', 'this', 'is', 'now', 'the', 'most', 'widely', 'used', 'phone', 'technology', 'on', 'the', 'planet', 'and', 'is', 'used', 'to', 'help', 'more', 'than', '1.2', 'billion', 'people', 'make', 'calls', '.', 'Mr', 'Caudwell', 'said', 'the', 'advent', 'of', 'digital', 'technology', 'also', 'helped', 'to', 'introduce', 'all', 'those', 'things', ',', 'such', 'as', 'text', 'messaging', 'and', 'roaming', 'that', 'have', 'made', 'mobiles', 'so', 'popular.Mobiles', 'rack', 'up', '20', 'years', 'of', 'use']\n"
     ]
    }
   ],
   "source": [
    "all_text = []\n",
    "for (i, j) in list(zip(description.values(), heading.values())):\n",
    "    all_text.append(word_tokenize(i + j))\n",
    "\n",
    "print(all_text[0])\n",
    "vocab_list = [i.lower() for j in all_text for i in j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def indexing(txt):\n",
    "    \"\"\"\n",
    "    The function creates word to index mapping\n",
    "    Param:\n",
    "        txt: List of Vocab words\n",
    "    \"\"\"\n",
    "    vocab = set(txt)\n",
    "    vocab_to_idx = {v:k for k, v in enumerate(vocab)}\n",
    "    idx_to_vocab = {v:k for k, v in vocab_to_idx.items()}\n",
    "    return vocab, vocab_to_idx, idx_to_vocab\n",
    "\n",
    "vocab, vocab_to_idx, idx_to_vocab = indexing(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267650\n",
      "17279\n",
      "17279\n",
      "511\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab_list))\n",
    "print(len(vocab))\n",
    "print(len(vocab_to_idx))\n",
    "print(len(all_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Reading in the pre-trained GloVe vectors from https://nlp.stanford.edu/projects/glove/** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(\"glove.6B.200d.txt\",\"rb\") as f:\n",
    "    glove = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating GloVe word and weight dictionary....\n",
      "Completed!\n"
     ]
    }
   ],
   "source": [
    "def glove_dict(glove_vector):\n",
    "    \"\"\"\n",
    "    The function creates mapping between words and the GloVe vectors\n",
    "    Param:\n",
    "        glove_vector: List of GloVe words and their weights\n",
    "    \"\"\"\n",
    "    word_weights = []\n",
    "    for word in glove_vector:\n",
    "        word_weights.append(word.split())\n",
    "    \n",
    "    print(\"Creating GloVe word and weight dictionary....\")\n",
    "    glove_words_weights = dict((i[0], i[1:]) for i in word_weights)\n",
    "    \n",
    "    print(\"Completed!\")\n",
    "    return glove_words_weights\n",
    "\n",
    "glove_words_weights = glove_dict(glove)\n",
    "pickle.dump(glove_words_weights, open('glove_words_weight.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GloVe symbols:  400000\n",
      "(400000, 200)\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "n_embeddings = 200\n",
    "n_glove_symbols = len(glove_words_weights.keys())\n",
    "print(\"Number of GloVe symbols: \", n_glove_symbols)\n",
    "glove_weight_matrix = np.empty((n_glove_symbols, n_embeddings))\n",
    "print(glove_weight_matrix.shape)\n",
    "print(glove_weight_matrix[:10,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "GloVe weight matrix std... 0.0381862057272\n",
      "[[-0.0071549   0.0093459   0.0023738  -0.0090339   0.0056123 ]\n",
      " [ 0.017651    0.029208   -0.00020768 -0.037523    0.00049139]\n",
      " [ 0.012289    0.058037   -0.0069635  -0.050288    0.010503  ]\n",
      " [ 0.0052924   0.025427    0.031353   -0.035613    0.0029629 ]\n",
      " [ 0.057346    0.05417    -0.023477   -0.03624     0.04037   ]]\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "glove_index_dict = {}\n",
    "global_scale = .1\n",
    "for i in glove_words_weights.keys():\n",
    "    w = i.decode(\"utf-8\")\n",
    "    glove_index_dict[w] = c \n",
    "    glove_weight_matrix[c,:] = glove_words_weights[i] \n",
    "    c += 1\n",
    "    if c % 100000 == 0:\n",
    "        print(c)\n",
    "        \n",
    "glove_weight_matrix *= global_scale\n",
    "print(\"GloVe weight matrix std...\", glove_weight_matrix.std())\n",
    "print(glove_weight_matrix[:5,:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale:  0.0935367192446\n",
      "Embedding shape... (17279, 200) Std... 0.0540163978217\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "shape = (vocab_size, n_embeddings)\n",
    "scale = glove_weight_matrix.std() * np.sqrt(12/2)\n",
    "print(\"Scale: \", scale)\n",
    "embedding = np.random.uniform(low=-scale, high=scale, size=shape)\n",
    "print(\"Embedding shape...\",embedding.shape,\"Std...\",embedding.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of tokens, found in GloVe matrix and copied to embedding... 14354 0.8307193703339314\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for i in range(vocab_size):\n",
    "    w = idx_to_vocab[i]\n",
    "    g = glove_index_dict.get(w, glove_index_dict.get(w))\n",
    "    if g is not None:\n",
    "        embedding[i,:] = glove_weight_matrix[g]\n",
    "        c +=1\n",
    "\n",
    "print(\"No. of tokens, found in GloVe matrix and copied to embedding...\", c, float(c / vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lots of words in the vocabulary not found in pre-trained GloVe dictionary. For those words, trying find the closest word in the GloVe dictionary** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14354"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_threshold = 0.5\n",
    "word2glove = {}\n",
    "for w in vocab_to_idx:\n",
    "    if w in glove_index_dict:\n",
    "        g = w\n",
    "        word2glove[w] = g\n",
    "\n",
    "len(word2glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of glove substitutes found 1678\n"
     ]
    }
   ],
   "source": [
    "normed_embedding = embedding/np.array([np.sqrt(np.dot(gweight,gweight)) for gweight in embedding])[:,None]\n",
    "\n",
    "nb_unknown_words = 2600\n",
    "\n",
    "glove_match = []\n",
    "for w, idx in vocab_to_idx.items():\n",
    "    if idx >= vocab_size-nb_unknown_words and w.isalpha() and w in word2glove:\n",
    "        gidx = glove_index_dict[word2glove[w]]\n",
    "        gweight = glove_weight_matrix[gidx,:].copy()\n",
    "        \n",
    "        # find row in embedding that has the highest cos score with gweight\n",
    "        gweight /= np.sqrt(np.dot(gweight,gweight))\n",
    "        score = np.dot(normed_embedding[:vocab_size-nb_unknown_words], gweight)\n",
    "        while True:\n",
    "            embedding_idx = score.argmax()\n",
    "            s = score[embedding_idx]\n",
    "            if s < glove_threshold:\n",
    "                break\n",
    "            if idx_to_vocab[embedding_idx] in word2glove :\n",
    "                glove_match.append((w, embedding_idx, s)) \n",
    "                break\n",
    "            score[embedding_idx] = -1\n",
    "glove_match.sort(key = lambda x: -x[2])\n",
    "print('# of glove substitutes found', len(glove_match))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manually checking the substitute words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.505118716841 dots => pixels\n",
      "0.504919795961 wallets => pockets\n",
      "0.504600572303 scratches => bumps\n",
      "0.504364499959 programmed => configured\n",
      "0.503212410739 issuers => card\n",
      "0.50306687316 speegle => nienhuis\n",
      "0.502371861575 podshow => bbci\n",
      "0.501750363598 mater => cornell\n",
      "0.500028319271 aus => x1\n",
      "0.500019282124 upstart => rival\n"
     ]
    }
   ],
   "source": [
    "for orig, sub, score in glove_match[-10:]:\n",
    "    print(score, orig, '=>', idx_to_vocab[sub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_idx2idx = dict((vocab_to_idx[w], embedding_idx) for  w, embedding_idx, _ in glove_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17279, 200)\n",
      "1678\n"
     ]
    }
   ],
   "source": [
    "print(embedding.shape)\n",
    "print(len(glove_match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word substitued with the closest word is... 1678\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for i in glove_idx2idx.keys():\n",
    "    g = glove_idx2idx[i]\n",
    "    embedding[i, :] = embedding[g]\n",
    "    c += 1\n",
    "\n",
    "print(\"Number of word substitued with the closest word is...\", c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building Encoder Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_builder(embeds):\n",
    "    model = keras.Sequential()\n",
    "    model.add(Embedding(weights=[embeds], name=\"embedding_1\", input_dim=vocab_size,\n",
    "                        output_dim=n_embeddings, input_length = max_length))\n",
    "    for i in range(2):\n",
    "        lstm = LSTM(rnn_size, name=\"layer_%s\" %(i), return_sequences=True)\n",
    "        model.add(lstm)\n",
    "        model.add(Dropout(prob, name=\"drop_%s\" %(i)))\n",
    "        \n",
    "    lstm = LSTM(rnn_size, name=\"layer_2\", return_sequences=False)\n",
    "    model.add(lstm)\n",
    "    model.add(Dropout(prob, name=\"drop_2\"))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('softmax', name=\"activation\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rnn_size = 200\n",
    "prob = 0.5\n",
    "encoder = model_builder(embedding)\n",
    "encoder.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoding document words with their the rescpective word indices (i.e. vocab_to_idx) and paddign them to make them of same lenght**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "desc_list = []\n",
    "for i in description.values():\n",
    "    desc_list.append(word_tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head_list = []\n",
    "for i in heading.values():\n",
    "    head_list.append(word_tokenize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mobiles', 'rack', 'up', '20', 'years', 'of', 'use']"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "511\n"
     ]
    }
   ],
   "source": [
    "doc2idx = []\n",
    "for i in desc_list:\n",
    "    doc2idx.append([vocab_to_idx[w.lower()] if w.lower() in vocab_to_idx.keys() else 0 for w in i])\n",
    "    \n",
    "print(len(doc2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1230, 8893, 17174, 5792, 10803, 359, 11990]\n"
     ]
    }
   ],
   "source": [
    "head2idx = []\n",
    "for i in head_list:\n",
    "    head2idx.append([vocab_to_idx[w.lower()] if w.lower() in vocab_to_idx.keys() else 0 for w in i])\n",
    "    \n",
    "print(head2idx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_length = 200\n",
    "padded_docs = pad_sequences(doc2idx, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(511, 200)"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_docs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting the Network defined on the Documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are passing a target array of shape (511, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-416-b8185a8f9520>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Krishnang/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/Krishnang/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1581\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1582\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1583\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Krishnang/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1427\u001b[0m         _check_loss_and_target_compatibility(y,\n\u001b[1;32m   1428\u001b[0m                                              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_loss_fns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m                                              self._feed_output_shapes)\n\u001b[0m\u001b[1;32m   1430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Krishnang/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_check_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 raise ValueError(\n\u001b[1;32m    285\u001b[0m                     \u001b[0;34m'You are passing a target array of shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0;34m' while using as loss `categorical_crossentropy`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                     \u001b[0;34m'`categorical_crossentropy` expects '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                     \u001b[0;34m'targets to be binary matrices (1s and 0s) '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are passing a target array of shape (511, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets."
     ]
    }
   ],
   "source": [
    "encoder.fit(padded_docs, head2idx, epochs=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
